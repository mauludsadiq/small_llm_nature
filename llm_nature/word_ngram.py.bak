from __future__ import annotations
import re
import math
from .char_ngram import NGramModel

_WORD_RE = re.compile(r"[A-Za-z0-9']+")

def tokenize(text: str) -> list[str]:
    return [m.group(0).lower() for m in _WORD_RE.finditer(text)]

def join_tokens(tokens: list[str]) -> str:
    return " ".join(tokens)

class WordNGram:
    """
    Word-level wrapper around the same NGramModel. Context keys are strings like "w1 w2 w3".
    """
    def __init__(self, k: int, alpha: float = 0.5):
        self.k = k
        self.model = NGramModel(k=k, alpha=alpha)

    def build_pairs(self, tokens: list[str]) -> list[tuple[str, str]]:
        pairs: list[tuple[str, str]] = []
        for i in range(len(tokens) - self.k):
            x = join_tokens(tokens[i:i+self.k])
            y = tokens[i+self.k]
            pairs.append((x, y))
        return pairs

    def fit_text(self, text: str) -> None:
        toks = tokenize(text)
        self.model.fit(self.build_pairs(toks))

    def log_prob_prompt_answer(self, q: str, a: str) -> float:
        prompt = f"q: {q} a: {a}".lower()
        toks = tokenize(prompt)
        pairs = self.build_pairs(toks)
        lp = 0.0
        for x, y in pairs:
            p = self.model.prob(x, y)
            if p <= 0.0:
                raise ValueError(f"Non-positive probability p={p} for ({x!r},{y!r})")
            lp += math.log(p)
        return lp
