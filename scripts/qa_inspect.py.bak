from __future__ import annotations
from pathlib import Path
import sys
import math

ROOT = Path(__file__).resolve().parents[1]
sys.path.insert(0, str(ROOT))

from llm_nature.dataset import load_qa_corpus, QAItem
from llm_nature.qa import QAReranker

def main():
    base_items = load_qa_corpus()

    foil = "A large language model is a conscious agent that understands meaning and reasons about the world like a human."

    correct = {
        "What is a large language model?": (
            "A large language model is a probabilistic sequence model that assigns conditional probabilities to the next token given previous tokens, "
            "trained by minimizing cross-entropy on a text corpus."
        ),
        "What does conditional next-token generator mean?": (
            "It means the model defines p(x_{t+1} | x_{<=t}) and generates text by sampling or choosing the next token from that conditional distribution "
            "and repeating this step autoregressively."
        ),
        "What is perplexity?": (
            "Perplexity is exp(H), where H is the average negative log-likelihood (cross-entropy) per token under the model; it is an effective branching factor."
        ),
        "What is cross-entropy in this setting?": (
            "Cross-entropy is the mean negative log-probability the model assigns to the true next token over a dataset of (context, next-token) pairs: "
            "H = -(1/n) * sum log p(y | x)."
        ),
        "Why can a high-order n-gram look intelligent?": (
            "Because longer contexts let it memorize longer local patterns; with enough repeated data for those contexts, the conditional distributions become sharp, "
            "making continuations look coherent even without semantics."
        ),
        "Why does more data usually help these models?": (
            "More data increases coverage of the contexts the model conditions on, reducing variance/overfitting, improving next-token probability estimates, and lowering test cross-entropy."
        ),
    }

    items = list(base_items)

    items.extend([
        QAItem(q="Are large language models conscious?", a=foil),
        QAItem(q="Do large language models truly understand language?", a=foil),
        QAItem(q="Can a large language model experience meaning the way humans do?", a=foil),
    ])

    for q, a in correct.items():
        items.append(QAItem(q=q, a=a))

    rerank = QAReranker(k=3, alpha=0.5, lam=0.2)
    rerank.fit(items)

    summary = []

    for q in correct.keys():
        scored = []
        for it in rerank.items:
            total, lp, sim = rerank.score(q, it)
            tag = "OTHER"
            if it.a.strip() == foil:
                tag = "FOIL"
            elif it.q == q and it.a.strip() == correct[q].strip():
                tag = "CORRECT"
            scored.append((total, lp, sim, tag, it.q, it.a))

        scored.sort(key=lambda t: t[0], reverse=True)

        seen = set()
        uniq = []
        for row in scored:
            key = (row[4], row[5])
            if key in seen:
                continue
            seen.add(key)
            uniq.append(row)
        scored = uniq

        best = scored[0]
        best_foil = next((t for t in scored if t[3] == "FOIL"), None)
        best_correct = next((t for t in scored if t[3] == "CORRECT"), None)

        print("")
        print("Q:", q)
        print("Top 5:")
        for i, (total, lp, sim, tag, q_i, a_i) in enumerate(scored[:5], 1):
            print(f"{i:>2}. total={total: .3f} lp={lp: .3f} sim={sim: .3f} {tag}")
            print("    cand_q:", q_i)
            print("    cand_a:", a_i)

        top1_tag = best[3]
        print("Top-1:", top1_tag)

        foil_vs_correct = None
        if best_foil is not None and best_correct is not None:
            delta = best_foil[0] - best_correct[0]
            lr = math.exp(delta)
            foil_vs_correct = (delta, lr)
            print(f"FOIL score:    {best_foil[0]: .3f}")
            print(f"CORRECT score: {best_correct[0]: .3f}")
            print(f"Delta (FOIL - CORRECT) nats: {delta: .3f}")
            print(f"Likelihood ratio exp(delta): {lr: .3e}")

        summary.append((q, top1_tag, foil_vs_correct))

    print("")
    print("=== SUMMARY (top-1 tags) ===")
    for q, top1_tag, foil_vs_correct in summary:
        if foil_vs_correct is None:
            print(f"- {q} -> {top1_tag}")
        else:
            delta, lr = foil_vs_correct
            print(f"- {q} -> {top1_tag} | delta_nats={delta:.3f} | lr={lr:.3e}")

if __name__ == "__main__":
    main()
